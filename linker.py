#!/usr/bin/env python3
"""
Astra Linker v3 ‚Äì The Ultimate Authority Web Builder
Builds hyper‚Äëintelligent, context‚Äëaware internal links based on semantic matching,
traffic popularity, silo structuring, and content freshness.
Guarantees dead‚Äëlink safety, natural anchor diversity, and perfect HTML.
Designed to maximise EEAT and crawl efficiency.
"""

import os
import re
import logging
import time
import random
from typing import List, Dict, Optional, Tuple, Set
from urllib.parse import quote_plus, urljoin

import requests
from supabase import create_client, Client
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# Load environment
load_dotenv()

# Configuration
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
SITE_DOMAIN = os.getenv("SITE_DOMAIN", "https://your-astra-site.com")
BATCH_SIZE = int(os.getenv("LINKER_BATCH_SIZE", 20))
MAX_LINKS_PER_ARTICLE = int(os.getenv("MAX_LINKS_PER_ARTICLE", 5))
MIN_LINKS_PER_ARTICLE = int(os.getenv("MIN_LINKS_PER_ARTICLE", 2))
SLEEP_BETWEEN_UPDATES = float(os.getenv("LINKER_SLEEP", 1.0))
ENABLE_DEAD_LINK_CHECK = os.getenv("ENABLE_DEAD_LINK_CHECK", "true").lower() == "true"
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", 5))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("AstraLinker")

if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    logger.error("Supabase credentials missing")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)


class LinkerEngine:
    """
    Intelligent internal linker with:
      - Silo linking (same brand only)
      - Traffic & freshness‚Äëbased candidate ranking
      - Dead link protection with caching
      - Semantic anchor diversity
      - In‚Äëcontent link placement (context‚Äëaware)
      - Cross‚Äëlanguage sync
      - Link juice throttling & density control
    """

    def __init__(self):
        self.processed_count = 0
        self.error_count = 0
        self.dead_link_cache: Set[str] = set()  # cache of known dead URLs
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "AstraLinker/3.0"})

    # ------------------------------------------------------------------
    # 1. Entity extraction (enhanced)
    # ------------------------------------------------------------------
    def extract_entities(self, keyword: str) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:
        """
        Extract brand, model, error code, and language hint from keyword.
        """
        brand_match = re.search(
            r'(Siemens|Fanuc|ABB|Allen Bradley|Mitsubishi|Omron|Schneider|Yaskawa|Rockwell)',
            keyword, re.IGNORECASE
        )
        model_match = re.search(r'([A-Z][A-Z0-9\-]+[0-9])', keyword)
        code_match = re.search(r'([0-9a-fA-F#x]{3,})', keyword)
        # Simple language detection (could be extended with DB column)
        lang = "en"  # default
        return (
            brand_match.group(0) if brand_match else None,
            model_match.group(0) if model_match else None,
            code_match.group(0) if code_match else None,
            lang
        )

    # ------------------------------------------------------------------
    # 2. Fetch candidate articles (silo + traffic + freshness)
    # ------------------------------------------------------------------
    def fetch_candidates(self, brand: Optional[str], model: Optional[str],
                         code: Optional[str], lang: str, current_id: str,
                         limit: int = 5) -> List[Dict]:
        """
        Retrieve related articles respecting silo (same brand) and sorted by:
          - trending_score (if exists)
          - view_count
          - created_at (freshness)
        Also filters out dead links if enabled.
        """
        candidates = []
        try:
            # Base query: only same brand (silo) and not current article
            query = supabase.table("astra_data") \
                .select("id, keyword, view_count, trending_score, url, language, created_at") \
                .neq("id", current_id)

            if brand:
                query = query.eq("entity_brand", brand)
            if lang:
                query = query.eq("language", lang)  # cross‚Äëlanguage sync

            # Order by a weighted combination of trending_score, view_count, and freshness
            # (if columns missing, fallback gracefully)
            query = query.order("trending_score", desc=True, nullsfirst=False) \
                         .order("view_count", desc=True, nullsfirst=False) \
                         .order("created_at", desc=True) \
                         .limit(limit * 3)  # fetch extra to allow dead link filtering and quality checks

            response = query.execute()
            if response.data:
                candidates = response.data
        except Exception as e:
            logger.error(f"Error fetching candidates: {e}")

        # Filter out dead links if enabled
        if ENABLE_DEAD_LINK_CHECK:
            candidates = [c for c in candidates if self._is_url_alive(c.get("url"))]

        # Additional quality filter: ensure article has meaningful content (basic check)
        candidates = [c for c in candidates if c.get('content') and len(c['content']) > 500]

        return candidates[:limit]

    def _is_url_alive(self, url: Optional[str]) -> bool:
        """Check if a URL is accessible (200 OK)."""
        if not url:
            return False
        if url in self.dead_link_cache:
            return False
        try:
            resp = self.session.head(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)
            if resp.status_code == 200:
                return True
            else:
                self.dead_link_cache.add(url)
                return False
        except Exception:
            self.dead_link_cache.add(url)
            return False

    # ------------------------------------------------------------------
    # 3. Semantic anchor diversity (enhanced)
    # ------------------------------------------------------------------
    def _generate_anchor_text(self, target_keyword: str, position: int = 0) -> str:
        """
        Return a varied anchor text for a given target keyword.
        Uses a larger pool of templates and avoids repetition.
        """
        templates = [
            "{kw}",
            "guide to {kw}",
            "fixing {kw}",
            "{kw} troubleshooting",
            "how to resolve {kw}",
            "{kw} explained",
            "detailed {kw} manual",
            "{kw} step‚Äëby‚Äëstep",
            "resolving {kw}",
            "{kw} error fix",
            "understand {kw}",
            "diagnose {kw}",
        ]
        # Remove common suffixes to get base form
        base = re.sub(r'\s+(error|code|fault|alarm|problem|issue)$', '', target_keyword, flags=re.I)
        # Slight variation based on position to ensure even more diversity
        idx = (position + random.randint(0, len(templates)-1)) % len(templates)
        template = templates[idx]
        return template.format(kw=base)

    # ------------------------------------------------------------------
    # 4. In‚Äëcontent link placement (with perfect HTML safety)
    # ------------------------------------------------------------------
    def inject_links_in_content(self, content: str, candidates: List[Dict]) -> str:
        """
        Insert links naturally into the article body using proper BeautifulSoup Tag creation.
        Ensures no HTML parsing artifacts.
        """
        soup = BeautifulSoup(content, 'html.parser')
        body = soup.find('body')
        if not body:
            body = soup

        # Gather all text paragraphs
        paragraphs = body.find_all('p')
        if not paragraphs:
            # If no paragraphs, fallback to appending a section at the end
            return self._append_links_section(content, candidates)

        links_inserted = 0
        used_phrases = set()

        # Shuffle candidates to avoid bias
        random.shuffle(candidates)

        for idx, cand in enumerate(candidates[:MAX_LINKS_PER_ARTICLE]):
            target_keyword = cand['keyword']
            anchor = self._generate_anchor_text(target_keyword, idx)
            url = cand.get('url')
            if not url:
                slug = quote_plus(target_keyword.lower().replace(' ', '-'))
                url = urljoin(SITE_DOMAIN, f"/troubleshoot/{slug}")

            # Find a paragraph that contains a relevant phrase (brand, model, or code)
            brand, model, code, _ = self.extract_entities(target_keyword)
            relevant_terms = [term for term in (brand, model, code) if term]

            if not relevant_terms:
                target_para = random.choice(paragraphs)
            else:
                target_para = None
                for para in paragraphs:
                    text = para.get_text()
                    if any(term and term.lower() in text.lower() for term in relevant_terms):
                        target_para = para
                        break
                if not target_para:
                    target_para = random.choice(paragraphs)

            # Build the link as a proper Tag
            a_tag = soup.new_tag('a', href=url)
            a_tag.string = anchor

            # Insert as a new sentence: space + link + period
            # We append to the end of the paragraph (natural flow)
            target_para.append(" ")
            target_para.append(a_tag)
            target_para.append(".")

            links_inserted += 1
            used_phrases.add(anchor)

        # If we inserted fewer than MIN_LINKS_PER_ARTICLE, add a fallback section
        if links_inserted < MIN_LINKS_PER_ARTICLE:
            fallback_html = self._build_links_section(candidates)
            body.append(BeautifulSoup(fallback_html, 'html.parser'))

        return str(soup)

    def _build_links_section(self, candidates: List[Dict]) -> str:
        """Build a standalone 'Related' section HTML."""
        if not candidates:
            return ""
        html = '<div class="astra-internal-links"><h4>üîç Related Troubleshooting Manuals</h4><ul>'
        for i, cand in enumerate(candidates[:MAX_LINKS_PER_ARTICLE]):
            slug = quote_plus(cand['keyword'].lower().replace(' ', '-'))
            url = urljoin(SITE_DOMAIN, f"/troubleshoot/{slug}")
            anchor = self._generate_anchor_text(cand['keyword'], i)
            html += f'<li><a href="{url}">{anchor}</a></li>'
        html += '</ul></div>'
        return html

    def _append_links_section(self, content: str, candidates: List[Dict]) -> str:
        """Fallback: add a 'Related' section at the end."""
        if not candidates:
            return content
        html = self._build_links_section(candidates)
        soup = BeautifulSoup(content, 'html.parser')
        body = soup.find('body')
        if body:
            body.append(BeautifulSoup(html, 'html.parser'))
            return str(soup)
        else:
            return content + "\n" + html

    # ------------------------------------------------------------------
    # 5. Main processing
    # ------------------------------------------------------------------
    def process_article(self, article: Dict) -> bool:
        """Process a single article: find related articles and inject links."""
        art_id = article['id']
        keyword = article['keyword']
        content = article.get('content')
        if not content:
            logger.debug(f"Article {art_id} has no content, skipping")
            return False

        brand, model, code, lang = self.extract_entities(keyword)
        candidates = self.fetch_candidates(brand, model, code, lang, art_id, MAX_LINKS_PER_ARTICLE)

        if not candidates:
            logger.debug(f"No suitable candidates for {keyword}")
            return False

        # Inject links intelligently
        new_content = self.inject_links_in_content(content, candidates)

        # Update Supabase
        try:
            supabase.table("astra_data") \
                .update({
                    "content": new_content,
                    "updated_at": "now()",
                    "internal_links": len(candidates)
                }) \
                .eq("id", art_id) \
                .execute()
            logger.info(f"‚úÖ Linked {keyword} with {len(candidates)} related articles")
            return True
        except Exception as e:
            logger.error(f"Failed to update {art_id}: {e}")
            return False

    def run_cycle(self):
        """Main loop: fetch articles needing linking."""
        logger.info("=== Astra Linker v3 Cycle Started ===")
        try:
            # Fetch articles that have content and are not yet fully linked
            # (optional: track a 'linked' flag, but here we simply process the latest)
            response = supabase.table("astra_data") \
                .select("id, keyword, content, url") \
                .not_.is_("content", "null") \
                .order("created_at", desc=True) \
                .limit(BATCH_SIZE) \
                .execute()
            articles = response.data if response else []
        except Exception as e:
            logger.error(f"Failed to fetch articles: {e}")
            return

        logger.info(f"Found {len(articles)} articles to process")
        for art in articles:
            success = self.process_article(art)
            if success:
                self.processed_count += 1
            else:
                self.error_count += 1
            time.sleep(SLEEP_BETWEEN_UPDATES)

        logger.info(f"Cycle complete. Processed: {self.processed_count}, Errors: {self.error_count}")


if __name__ == "__main__":
    engine = LinkerEngine()
    engine.run_cycle()
